{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'report1.csv' has been created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Define the directory containing your text files\n",
    "txt_dir = \"/Users/pavankonam/Downloads/Dataset for EEG/New Database/Englishs-1\"\n",
    "csv_file = \"report1.csv\"\n",
    "\n",
    "# Get all txt files in the directory\n",
    "txt_files = [f for f in os.listdir(txt_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "# Write to CSV\n",
    "# Write to CSV\n",
    "with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Filename\", \"Content\"])  # Header row\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        txt_path = os.path.join(txt_dir, txt_file)\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()  # Read and clean up content\n",
    "        \n",
    "        writer.writerow([txt_file, content]) \n",
    "print(f\"CSV file '{csv_file}' has been created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'report4.csv' has been created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Define the directory containing your text files\n",
    "txt_dir = \"/Users/pavankonam/Downloads/Dataset for EEG/New Database/Reports-4\"\n",
    "csv_file = \"report4.csv\"\n",
    "\n",
    "# Get all txt files in the directory\n",
    "txt_files = [f for f in os.listdir(txt_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "# Write to CSV\n",
    "# Write to CSV\n",
    "with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Filename\", \"Content\"])  # Header row\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        txt_path = os.path.join(txt_dir, txt_file)\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()  # Read and clean up content\n",
    "        \n",
    "        writer.writerow([txt_file, content]) \n",
    "print(f\"CSV file '{csv_file}' has been created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the files of subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'report3.csv' has been created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Define the main directory containing subfolders with text files\n",
    "main_dir = \"/Users/pavankonam/Downloads/Dataset for EEG/New Database/English-3\"\n",
    "csv_file = \"report3.csv\"\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Filename\", \"Content\"])  # Header row\n",
    "    \n",
    "    # Walk through all subdirectories and files\n",
    "    for root, _, files in os.walk(main_dir):\n",
    "        for txt_file in files:\n",
    "            if txt_file.endswith(\".txt\"):\n",
    "                txt_path = os.path.join(root, txt_file)\n",
    "                \n",
    "                with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read().strip()  # Read and clean up content\n",
    "                \n",
    "                # Store relative path for clarity\n",
    "                relative_path = os.path.relpath(txt_path, main_dir)\n",
    "                \n",
    "                writer.writerow([relative_path, content])  # Write filename and content\n",
    "\n",
    "print(f\"CSV file '{csv_file}' has been created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the information from the docx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'report2.csv' has been created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import docx  # Import the python-docx library\n",
    "\n",
    "# Define the main directory containing subfolders with docx files\n",
    "main_dir = \"/Users/pavankonam/Downloads/Dataset for EEG/New Database/Reports-2\"\n",
    "csv_file = \"report2.csv\"\n",
    "\n",
    "# Function to extract text from a .docx file\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = docx.Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs]).strip()\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Filename\", \"Content\"])  # Header row\n",
    "    \n",
    "    # Walk through all subdirectories and files\n",
    "    for root, _, files in os.walk(main_dir):\n",
    "        for docx_file in files:\n",
    "            if docx_file.endswith(\".docx\"):\n",
    "                docx_path = os.path.join(root, docx_file)\n",
    "                \n",
    "                content = extract_text_from_docx(docx_path)  # Extract content\n",
    "                \n",
    "                # Store relative path for clarity\n",
    "                relative_path = os.path.relpath(docx_path, main_dir)\n",
    "                \n",
    "                writer.writerow([relative_path, content])  # Write filename and content\n",
    "\n",
    "print(f\"CSV file '{csv_file}' has been created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CSV file 'final_output.csv' has been created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "csv_dir = \"/Users/pavankonam/Downloads/Dataset for EEG\"\n",
    "final_csv = \"final_output.csv\"\n",
    "\n",
    "# Get all CSV files in the directory\n",
    "csv_files = [os.path.join(csv_dir, f) for f in os.listdir(csv_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "# Merge all CSV files\n",
    "df_list = [pd.read_csv(csv_file) for csv_file in csv_files]\n",
    "final_df = pd.concat(df_list, ignore_index=True)  # Combine into one DataFrame\n",
    "\n",
    "# Save to final CSV file\n",
    "final_df.to_csv(final_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Final CSV file '{final_csv}' has been created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have 5 distinct categories \n",
    "1. Adventure & Movement\n",
    "2. Fear and Uncertainity\n",
    "3. People & Social Interaction\n",
    "4. Abstract & thought based\n",
    "5. Miscellaneous & unclear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDF file list created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"/Users/pavankonam/Downloads/Dataset for EEG/New Database/EEG-waves\"\n",
    "edf_files = [f for f in os.listdir(folder_path) if f.endswith('.edf')]\n",
    "\n",
    "# Save to text file\n",
    "with open(\"edf_file_list.txt\", \"w\") as f:\n",
    "    for file in edf_files:\n",
    "        f.write(file + \"\\n\")\n",
    "\n",
    "print(\"EDF file list created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file saved as 'combined_output.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel files, specifying that headers are in the second row (index 1)\n",
    "file1 = pd.read_excel('/Users/pavankonam/Downloads/Dataset for EEG/New Database/final_output.xls', header=1)\n",
    "file2 = pd.read_excel('/Users/pavankonam/Downloads/Dataset for EEG/New Database/Reports.xls', header=1)\n",
    "\n",
    "# Define the columns to extract (same for both files)\n",
    "columns_to_extract = ['Filename', 'Category']\n",
    "\n",
    "# Function to extract columns safely\n",
    "def extract_columns(df, columns):\n",
    "    # Find columns that exist in both the expected list and the DataFrame\n",
    "    existing_columns = [col for col in columns if col in df.columns]\n",
    "    \n",
    "    if not existing_columns:\n",
    "        print(\"Error: None of the expected columns found in the file\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Show warning if some columns are missing\n",
    "    missing_columns = set(columns) - set(existing_columns)\n",
    "    if missing_columns:\n",
    "        print(f\"Warning: Columns {missing_columns} not found in this file\")\n",
    "    \n",
    "    return df[existing_columns]\n",
    "\n",
    "# Extract data from both files\n",
    "df1 = extract_columns(file1, columns_to_extract)\n",
    "df2 = extract_columns(file2, columns_to_extract)\n",
    "\n",
    "# Check if both DataFrames have data\n",
    "if df1.empty or df2.empty:\n",
    "    print(\"Error: One or both files don't contain the required columns\")\n",
    "else:\n",
    "    # Combine the DataFrames\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    combined_df.to_csv('combined_output.csv', index=False)\n",
    "    print(\"Combined CSV file saved as 'combined_output.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. Modified file saved as 'modified_file.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/Users/pavankonam/Downloads/Dataset for EEG/combined_output.csv')\n",
    "\n",
    "# Function to convert .txt to .edf\n",
    "def convert_extension(filename):\n",
    "    if filename.endswith('.txt'):\n",
    "        return filename[:-4] + '.edf'\n",
    "    else:\n",
    "        return filename\n",
    "\n",
    "# Apply the conversion to the 'Filename' column\n",
    "df['Filename'] = df['Filename'].apply(convert_extension)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv('modified_file.csv', index=False)\n",
    "\n",
    "print(\"Conversion complete. Modified file saved as 'modified_file.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'edf_file_list.txt' contains 711 lines.\n"
     ]
    }
   ],
   "source": [
    "def count_lines(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            return len(lines)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{filename}' was not found.\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Example usage\n",
    "file_path = 'edf_file_list.txt'  # Replace with your file path\n",
    "line_count = count_lines(file_path)\n",
    "\n",
    "if line_count > 0:\n",
    "    print(f\"The file '{file_path}' contains {line_count} lines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All filenames in CSV are present in TXT\n",
      "\n",
      "All filenames in TXT are present in CSV\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV file\n",
    "csv_filename = 'modified_file.csv'  # Replace with your CSV file path\n",
    "csv_data = pd.read_csv(csv_filename)\n",
    "csv_filenames = set(csv_data['Filename'].dropna())  # Convert to set and remove NaN values\n",
    "\n",
    "# Read TXT file\n",
    "txt_filename = 'edf_file_list.txt'  # Replace with your TXT file path\n",
    "with open(txt_filename, 'r') as file:\n",
    "    txt_filenames = set(line.strip() for line in file.readlines())\n",
    "\n",
    "# Find filenames that are in CSV but not in TXT\n",
    "missing_in_txt = csv_filenames - txt_filenames\n",
    "\n",
    "# Find filenames that are in TXT but not in CSV\n",
    "missing_in_csv = txt_filenames - csv_filenames\n",
    "\n",
    "# Print results\n",
    "if missing_in_txt:\n",
    "    print(\"Filenames present in CSV but not in TXT:\")\n",
    "    for name in missing_in_txt:\n",
    "        print(f\"- {name}\")\n",
    "else:\n",
    "    print(\"All filenames in CSV are present in TXT\")\n",
    "listsofedf = []\n",
    "if missing_in_csv:\n",
    "    print(\"\\nFilenames present in TXT but not in CSV:\")\n",
    "    for name in missing_in_csv:\n",
    "        listsofedf.append(name)\n",
    "        print(f\"- {name}\")\n",
    "else:\n",
    "    print(\"\\nAll filenames in TXT are present in CSV\")\n",
    "print(len(missing_in_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DL050_night06.edf', 'DL021_night04.edf', 'DL006_night06.edf', 'case06_sub105.edf', 'DL006_night01.edf', 'DL019_night02.edf', 'DL048_night05.edf', 'DL027_night13.edf', 'DL008_night01.edf', 'DL018_night05.edf']\n"
     ]
    }
   ],
   "source": [
    "print(listsofedf[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: DL050_night06.edf\n",
      "Deleted: DL021_night04.edf\n",
      "Deleted: DL006_night06.edf\n",
      "Deleted: case06_sub105.edf\n",
      "Deleted: DL006_night01.edf\n",
      "Deleted: DL019_night02.edf\n",
      "Deleted: DL048_night05.edf\n",
      "Deleted: DL027_night13.edf\n",
      "Deleted: DL008_night01.edf\n",
      "Deleted: DL018_night05.edf\n",
      "Deleted: DL080_night04.edf\n",
      "Deleted: DL073_night04.edf\n",
      "Deleted: DL048_night08.edf\n",
      "Deleted: DL006_night09.edf\n",
      "Deleted: DL080_night07.edf\n",
      "Deleted: DL042_night09.edf\n",
      "Deleted: DL019_night11.edf\n",
      "Deleted: DL019_night05.edf\n",
      "Deleted: DL019_night06.edf\n",
      "Deleted: DL048_night11.edf\n",
      "Deleted: DL080_night09.edf\n",
      "Deleted: DL042_night03.edf\n",
      "Deleted: DL073_night08.edf\n",
      "Deleted: DL005_night13.edf\n",
      "Deleted: DL078_night09.edf\n",
      "Deleted: DL048_night14.edf\n",
      "Deleted: DL008_night09.edf\n",
      "Deleted: DL030_night06.edf\n",
      "Deleted: DL055_night10.edf\n",
      "Deleted: DL016_night08.edf\n",
      "Deleted: DL050_night13.edf\n",
      "Deleted: DL055_night01.edf\n",
      "Deleted: DL024_night10.edf\n",
      "Deleted: DL042_night14.edf\n",
      "Deleted: DL078_night13.edf\n",
      "Deleted: DL019_night04.edf\n",
      "Deleted: DL034_night11.edf\n",
      "Deleted: DL042_night06.edf\n",
      "Deleted: DL006_night05.edf\n",
      "Deleted: DL070_night01.edf\n",
      "Deleted: DL049_night05.edf\n",
      "Deleted: DL080_night03.edf\n",
      "Deleted: DL048_night04.edf\n",
      "Deleted: DL073_night15.edf\n",
      "Deleted: DL086_night12.edf\n",
      "Deleted: DL048_night06.edf\n",
      "Deleted: DL080_night02.edf\n",
      "Deleted: DL021_night10.edf\n",
      "Deleted: DL080_night14.edf\n",
      "Deleted: DL019_night07.edf\n",
      "Deleted: DL034_night10.edf\n",
      "Deleted: DL008_night05.edf\n",
      "Deleted: DL034_night06.edf\n",
      "Deleted: DL019_night08.edf\n",
      "Deleted: DL008_night02.edf\n",
      "Deleted: DL027_night11.edf\n",
      "Deleted: DL073_night01.edf\n",
      "Deleted: DL005_night06.edf\n",
      "Deleted: DL073_night05.edf\n",
      "Deleted: DL049_night14.edf\n",
      "Deleted: DL005_night03.edf\n",
      "Deleted: DL008_night07.edf\n",
      "Deleted: DL008_night04.edf\n",
      "Deleted: DL048_night01.edf\n",
      "\n",
      "Summary:\n",
      "Successfully deleted 64 files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def delete_files_from_list(folder_path, file_list):\n",
    "    deleted_files = []\n",
    "    missing_files = []\n",
    "    \n",
    "    for filename in file_list:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                deleted_files.append(filename)\n",
    "                print(f\"Deleted: {filename}\")\n",
    "            except PermissionError:\n",
    "                print(f\"Permission denied for: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {filename}: {e}\")\n",
    "        else:\n",
    "            missing_files.append(filename)\n",
    "    \n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Successfully deleted {len(deleted_files)} files.\")\n",
    "    if missing_files:\n",
    "        print(f\"{len(missing_files)} files not found:\")\n",
    "        for file in missing_files:\n",
    "            print(f\"- {file}\")\n",
    "    return deleted_files, missing_files\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your folder path\n",
    "    folder_path = \"/Users/pavankonam/Downloads/Dataset for EEG/New Database/EEG-waves\"\n",
    "    \n",
    "    # Replace with your list of filenames\n",
    "    \n",
    "    delete_files_from_list(folder_path, listsofedf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
